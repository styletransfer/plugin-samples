{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KjTHAV8FgEza"
      },
      "outputs": [],
      "source": [
        "# Copyright 2025 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdL4uvQQs76x"
      },
      "source": [
        "# Veo 3 Video Generation\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/vision/getting-started/veo3_video_generation.ipynb\">\n",
        "      <img src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fvision%2Fgetting-started%2Fveo3_video_generation.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Run in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/vision/getting-started/veo3_video_generation.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>    \n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/vision/getting-started/veo3_video_generation.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "</table>\n",
        "\n",
        "<div style=\"clear: both;\"></div>\n",
        "\n",
        "<b>Share to:</b>\n",
        "\n",
        "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/vision/getting-started/veo3_video_generation.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/vision/getting-started/veo3_video_generation.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/vision/getting-started/veo3_video_generation.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/vision/getting-started/veo3_video_generation.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/vision/getting-started/veo3_video_generation.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUCUMoTmN_lJ"
      },
      "source": [
        "| | |\n",
        "|-|-|\n",
        "|Author(s) | [Katie Nguyen](https://github.com/katiemn) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDjAqcgigwdX"
      },
      "source": [
        "## Overview\n",
        "\n",
        "### Veo 3\n",
        "\n",
        "Veo 3 on Vertex AI gives application developers access to Google's cutting-edge video generation. This model creates videos with stunning detail and realistic physics across a wide array of visual styles. Veo 3 enhances video quality from text and image prompts, and now includes dialogue and audio generation.\n",
        "\n",
        "In this tutorial, you will learn how to use the Google Gen AI SDK for Python to interact with Veo 3 and generate new videos with audio from:\n",
        "- Enhanced text prompts\n",
        "- Starting input images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2_iOv5uhXVg"
      },
      "source": [
        "## Get started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4uerc9Xhf1f"
      },
      "source": [
        "### Install Google Gen AI SDK for Python"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PROJECT_ID:** lbw-leotoronto-tst-mg"
      ],
      "metadata": {
        "id": "5bcEmdsTGBpX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rJyFNKoQhiwF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f5318ea-e121-4e73-e1eb-cb59283d69bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/226.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.8/226.8 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade --quiet google-genai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Project Section\n",
        "# 1. Install necessary libraries\n",
        "# We add google-cloud-aiplatform for robust interaction with Vertex AI.\n",
        "!pip install pillow google-cloud-aiplatform --quiet\n",
        "\n",
        "# 2. Import libraries and handle authentication\n",
        "import google.auth\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "from IPython.display import display\n",
        "\n",
        "# This will trigger a pop-up in your notebook to authenticate your Google account.\n",
        "# It automatically gets the credentials needed for the client.\n",
        "credentials, project_id = google.auth.default()\n",
        "\n",
        "# --- Configuration ---\n",
        "# Your Project ID is set here.\n",
        "PROJECT_ID = \"lbw-leotoronto-tst-mg\"\n",
        "# 'us-central1' is a region where Imagen models are reliably available.\n",
        "LOCATION = \"us-central1\"\n",
        "\n",
        "print(f\"Using Project ID: {PROJECT_ID}\")\n",
        "print(f\"Using Location:   {LOCATION}\")\n",
        "\n",
        "# 3. Enable the Vertex AI API (this is a critical step)\n",
        "# It's safe to run this even if it's already enabled.\n",
        "print(\"Enabling the Vertex AI API...\")\n",
        "!gcloud services enable aiplatform.googleapis.com --project {PROJECT_ID}\n",
        "\n",
        "# 4. Initialize the client specifically for Vertex AI\n",
        "# --- THE FIX IS HERE ---\n",
        "# You MUST include `vertexai=True` to switch the client to Vertex AI mode.\n",
        "client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION, credentials=credentials)"
      ],
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDvBRxoq4ZM7",
        "outputId": "9d1698d1-7fde-479c-8b30-c7fb0a570b89"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mUsing Project ID: lbw-leotoronto-tst-mg\n",
            "Using Location:   us-central1\n",
            "Enabling the Vertex AI API...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bqz5LUG6h8fA",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Authenticate your notebook environment (Colab only)\n",
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVrasKoriKZn"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMQf_BkyiMgF"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "from IPython.display import Markdown, Video, display\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "import matplotlib.image as img\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qD_bwA9hiMzL"
      },
      "source": [
        "### Define a helper function to display media"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GUrEwbvFiPhJ"
      },
      "outputs": [],
      "source": [
        "def show_video(video):\n",
        "    if isinstance(video, str):\n",
        "        file_name = video.split(\"/\")[-1]\n",
        "        !gsutil cp {video} {file_name}\n",
        "        display(Video(file_name, embed=True, width=600))\n",
        "    else:\n",
        "        with open(\"sample.mp4\", \"wb\") as out_file:\n",
        "            out_file.write(video)\n",
        "        display(Video(\"sample.mp4\", embed=True, width=600))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jaSOOadiUj6"
      },
      "source": [
        "### Load the video generation model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "APRfTklCiYR2"
      },
      "outputs": [],
      "source": [
        "video_model = \"veo-3.0-generate-preview\"\n",
        "video_model_fast = \"veo-3.0-fast-generate-preview\"\n",
        "gemini_model = \"gemini-2.5-flash\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_R1_Y76i4QB"
      },
      "source": [
        "## Generate videos\n",
        "Now, you'll generate videos from text and/or image prompts. You can get started with your own prompts or complete the next section to optimize your prompts with some established best practices.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9ERu5b3EEdp"
      },
      "source": [
        "### [Optional] Optimize your prompt: Text-to-video\n",
        "\n",
        "By considering the following options in your prompt, you can use Veo to create higher quality videos that more closely resemble your desired outcome. Learn more about advanced [prompting techniques for Veo 3](https://medium.com/google-cloud/veo-3-a-detailed-prompting-guide-867985b46018). To get started specify the following, or leave them as `None` if they don't align with your specific goals.\n",
        "- **Subject:** The \"who\" or \"what\" of your video\n",
        "- **Action:** Describe movements, interactions, etc.\n",
        "- **Scene:** The \"where\" and \"when\" of your video\n",
        "- **Camera angles:** The shot's viewpoint\n",
        "- **Camera movements:** For a more cinematic/dynamic experience\n",
        "- **Lens effects:** How the camera \"sees\" the world\n",
        "- **Style:** The video's artistic filter\n",
        "- **Temporal elements:** To imply changes in time\n",
        "- **Audio:** Various sound effects or dialogue that guides the visuals through sound"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "yx7DKfqri3Vf",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "subject = \"a detective\"  # @param {type: 'string'}\n",
        "action = \"interrogating a rubber duck\"  # @param {type: 'string'}\n",
        "scene = \"in an interview room\"  # @param {type: 'string'}\n",
        "\n",
        "camera_angle = \"Over-the-Shoulder Shot\"  # @param [\"None\", \"Eye-Level Shot\", \"Low-Angle Shot\", \"High-Angle Shot\", \"Bird's-Eye View\", \"Top-Down Shot\", \"Worm's-Eye View\", \"Dutch Angle\", \"Canted Angle\", \"Close-Up\", \"Extreme Close-Up\", \"Medium Shot\", \"Full Shot\", \"Long Shot\", \"Wide Shot\", \"Establishing Shot\", \"Over-the-Shoulder Shot\", \"Point-of-View (POV) Shot\"]\n",
        "camera_movement = \"Zoom (In)\"  # @param [\"None\", \"Static Shot (or fixed)\", \"Pan (left)\", \"Pan (right)\", \"Tilt (up)\", \"Tilt (down)\", \"Dolly (In)\", \"Dolly (Out)\", \"Zoom (In)\", \"Zoom (Out)\", \"Truck (Left)\", \"Truck (Right)\", \"Pedestal (Up)\", \"Pedestal (Down)\", \"Crane Shot\", \"Aerial Shot\", \"Drone Shot\", \"Handheld\", \"Shaky Cam\", \"Whip Pan\", \"Arc Shot\"]\n",
        "lens_effects = \"None\"  # @param [\"None\", \"Wide-Angle Lens (e.g., 24mm)\", \"Telephoto Lens (e.g., 85mm)\", \"Shallow Depth of Field\", \"Bokeh\", \"Deep Depth of Field\", \"Lens Flare\", \"Rack Focus\", \"Fisheye Lens Effect\", \"Vertigo Effect (Dolly Zoom)\"]\n",
        "style = \"Cinematic\"  # @param [\"None\", \"Photorealistic\", \"Cinematic\", \"Vintage\", \"Japanese anime style\", \"Claymation style\", \"Stop-motion animation\", \"In the style of Van Gogh\", \"Surrealist painting\", \"Monochromatic black and white\", \"Vibrant and saturated\", \"Film noir style\", \"High-key lighting\", \"Low-key lighting\", \"Golden hour glow\", \"Volumetric lighting\", \"Backlighting to create a silhouette\"]\n",
        "temporal_elements = \"None\"  # @param [\"None\", \"Slow-motion\", \"Fast-paced action\", \"Time-lapse\", \"Hyperlapse\", \"Pulsating light\", \"Rhythmic movement\"]\n",
        "\n",
        "sound_effects = \"Ticking clock\"  # @param [\"None\", \"Sound of a phone ringing\", \"Water splashing\", \"Soft house sounds\", \"Ticking clock\", \"City traffic and sirens\", \"Waves crashing\", \"Quiet office hum\"]\n",
        "dialogue = \"Where were you last night?\"  # @param {type: 'string'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMFj078dLG0D"
      },
      "source": [
        "Now, you'll use Gemini to take all of these keywords and combine them into a detailed Veo prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "f3jvksummytE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "outputId": "3238ef91-ef02-4d54-f9b3-651ec7a17e73"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'Markdown' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-8-1122286528.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Set Gemini's response in a prompt variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMarkdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'Markdown' is not defined"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\n",
        "\n",
        "keywords = [subject, action, scene]\n",
        "optional_keywords = [\n",
        "    camera_angle,\n",
        "    camera_movement,\n",
        "    lens_effects,\n",
        "    style,\n",
        "    temporal_elements,\n",
        "    sound_effects,\n",
        "]\n",
        "for keyword in optional_keywords:\n",
        "    if keyword != \"None\":\n",
        "        keywords.append(keyword)\n",
        "if dialogue != \"\":\n",
        "    keywords.append(dialogue)\n",
        "\n",
        "gemini_prompt = f\"\"\"\n",
        "You are an expert video prompt engineer for Google's Veo model. Your task is to construct the most effective and optimal prompt string using the following keywords. Every single keyword MUST be included. Synthesize them into a single, cohesive, and cinematic instruction. Do not add any new core concepts. Output ONLY the final prompt string, without any introduction or explanation. Mandatory Keywords: {\",\".join(keywords)}\n",
        "\"\"\"\n",
        "response = client.models.generate_content(\n",
        "    model=gemini_model,\n",
        "    contents=gemini_prompt,\n",
        ")\n",
        "\n",
        "# Set Gemini's response in a prompt variable\n",
        "prompt = response.text\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDaTx8WCidRG"
      },
      "source": [
        "### Generate videos from a text prompt\n",
        "\n",
        "With Veo 3, you have the option to generate 8 second videos from a text prompt. In order to generate a video in the following sample, specify the following info:\n",
        "- **Prompt:** A detailed description of the video you would like to see. Only edit the prompt if you didn't generate a detailed prompt with Gemini in the previous section.\n",
        "- **Prompt enhancement:** The model offers the option to enhance your provided prompt.\n",
        "- **Audio generation:** Set `generate_audio` to True if you'd like audio to be included in the output video.\n",
        "- **Aspect ratio:** 16:9\n",
        "- **Number of videos:** Set this value to 1 or 2.\n",
        "- **Video duration:** 8 seconds"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install --upgrade google-cloud-aiplatform"
      ],
      "metadata": {
        "id": "IBu5APMfD4CC"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import base64\n",
        "import time\n",
        "import vertexai\n",
        "from vertexai.preview import generative_models\n",
        "from vertexai.preview.vision_models import Image, Video\n",
        "from vertexai.preview.vision_models import VideoGenerationModel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "id": "OOOOtFng9Zr4",
        "outputId": "bd0e1ad1-8913-4709-a95b-808589542d08"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'VideoGenerationModel' from 'vertexai.preview.vision_models' (/usr/local/lib/python3.11/dist-packages/vertexai/preview/vision_models.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-16-3900978672.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvertexai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreview\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgenerative_models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvertexai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreview\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvision_models\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVideo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mvertexai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreview\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvision_models\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVideoGenerationModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'VideoGenerationModel' from 'vertexai.preview.vision_models' (/usr/local/lib/python3.11/dist-packages/vertexai/preview/vision_models.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "W_iqdJNGl2Sa"
      },
      "outputs": [],
      "source": [
        "if prompt == \"\":\n",
        "    prompt = \"a garden gnome singing a pop song in a whimsical outdoor garden\"  # @param {type: 'string'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "tygfLLlWyTo_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "9f746b5c-b0bc-4b05-99f0-31a2fba68e4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cinematic Over-the-Shoulder Shot, zooming in slowly on a detective intensely interrogating a rubber duck in a stark interview room. An ominous ticking clock echoes in the background as the detective sternly asks, \"Where were you last night?\"\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'time' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-11-2068609741.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moperation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0moperation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moperations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'time' is not defined"
          ]
        }
      ],
      "source": [
        "print(prompt)\n",
        "enhance_prompt = True  # @param {type: 'boolean'}\n",
        "generate_audio = True  # @param {type: 'boolean'}\n",
        "\n",
        "operation = client.models.generate_videos(\n",
        "    model=video_model,\n",
        "    prompt=prompt,\n",
        "    config=types.GenerateVideosConfig(\n",
        "        aspect_ratio=\"16:9\",\n",
        "        number_of_videos=1,\n",
        "        duration_seconds=8,\n",
        "        person_generation=\"allow_adult\",\n",
        "        enhance_prompt=enhance_prompt,\n",
        "        generate_audio=generate_audio,\n",
        "    ),\n",
        ")\n",
        "\n",
        "while not operation.done:\n",
        "    time.sleep(8)\n",
        "    operation = client.operations.get(operation)\n",
        "    print(operation)\n",
        "\n",
        "if operation.response:\n",
        "    show_video(operation.result.generated_videos[0].video.video_bytes)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "eLJVX2jy9hjk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time # <--- ADD THIS LINE\n",
        "\n",
        "# This is the original prompt from the notebook.\n",
        "# You can uncomment and edit it to generate your own video.\n",
        "# prompt = \"Cinematic Over-the-Shoulder Shot, zooming in slowly on a detective intensely interrogating a rubber duck in a stark interview room. An ominous ticking clock echoes in the background as the detective sternly asks, \\\"Where were you last night?\\\"\"\n",
        "if 'prompt' not in locals() or prompt == \"\":\n",
        "    prompt = \"a garden gnome singing a pop song in a whimsical outdoor garden\"\n",
        "\n",
        "print(f\"Using prompt: {prompt}\")\n",
        "\n",
        "enhance_prompt = True\n",
        "generate_audio = True\n",
        "\n",
        "operation = client.models.generate_videos(\n",
        "    model=video_model,\n",
        "    prompt=prompt,\n",
        "    config=types.GenerateVideosConfig(\n",
        "        aspect_ratio=\"16:9\",\n",
        "        number_of_videos=1,\n",
        "        duration_seconds=8,\n",
        "        person_generation=\"allow_adult\",\n",
        "        enhance_prompt=enhance_prompt,\n",
        "        generate_audio=generate_audio,\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(\"Video generation started. This may take a few minutes.\")\n",
        "while not operation.done:\n",
        "    time.sleep(8)\n",
        "    operation = client.operations.get(operation)\n",
        "    print(\"...\") # Print something to show it's working\n",
        "\n",
        "print(\"Operation finished.\")\n",
        "\n",
        "if operation.response:\n",
        "    show_video(operation.result.generated_videos[0].video.video_bytes)\n",
        "else:\n",
        "    print(\"Operation failed or returned no response.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        },
        "id": "xgQWTLRU9iZ9",
        "outputId": "8c7b9aa5-adcc-4199-9592-d8c3c2555329"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using prompt: Cinematic Over-the-Shoulder Shot, zooming in slowly on a detective intensely interrogating a rubber duck in a stark interview room. An ominous ticking clock echoes in the background as the detective sternly asks, \"Where were you last night?\"\n",
            "Video generation started. This may take a few minutes.\n",
            "...\n",
            "...\n",
            "...\n",
            "...\n",
            "...\n",
            "...\n",
            "...\n",
            "...\n",
            "...\n",
            "...\n",
            "...\n",
            "Operation finished.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Video.__init__() got an unexpected keyword argument 'embed'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-13-182002271.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moperation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mshow_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerated_videos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvideo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvideo_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Operation failed or returned no response.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-5-2021470259.py\u001b[0m in \u001b[0;36mshow_video\u001b[0;34m(video)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sample.mp4\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mout_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mout_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVideo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sample.mp4\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: Video.__init__() got an unexpected keyword argument 'embed'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "688nb6GEwqR4"
      },
      "source": [
        "- **Veo 3 Fast:** For this next example, you'll use Veo 3 Fast. This model is a great option for use cases where latency is a priority over maximum quality.\n",
        "- **File location:** In this request, the video will be stored in Cloud Storage once video generation is complete. Specify the bucket path where you would like this video to be stored in the `output_gcs` field.\n",
        "- **Person generation:** When generating videos of people you can also set the `person_generation` parameter accordingly: `allow_adult`, `dont_allow`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sp0K0WYUwxLJ"
      },
      "outputs": [],
      "source": [
        "prompt = \"sculpting a bowl on a pottery wheel while the person sculpting remarks: wow, I love this one\"  # @param {type: 'string'}\n",
        "output_gcs = \"gs://[your-bucket-path]\"  # @param {type: 'string'}\n",
        "enhance_prompt = True  # @param {type: 'boolean'}\n",
        "generate_audio = True  # @param {type: 'boolean'}\n",
        "\n",
        "\n",
        "operation = client.models.generate_videos(\n",
        "    model=video_model_fast,\n",
        "    prompt=prompt,\n",
        "    config=types.GenerateVideosConfig(\n",
        "        aspect_ratio=\"16:9\",\n",
        "        output_gcs_uri=output_gcs,\n",
        "        number_of_videos=1,\n",
        "        duration_seconds=8,\n",
        "        person_generation=\"allow_adult\",\n",
        "        enhance_prompt=enhance_prompt,\n",
        "        generate_audio=generate_audio,\n",
        "    ),\n",
        ")\n",
        "\n",
        "while not operation.done:\n",
        "    time.sleep(15)\n",
        "    operation = client.operations.get(operation)\n",
        "    print(operation)\n",
        "\n",
        "if operation.response:\n",
        "    show_video(operation.result.generated_videos[0].video.uri)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YysYLyiVj8Zd"
      },
      "source": [
        "### Generate videos from an image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-NdoBONKpJD"
      },
      "source": [
        "#### Download the starting image\n",
        "\n",
        "You can also generate a video by starting with an input image. In this example, you'll locally download an image that's stored in Google Cloud Storage. If you'd like, you can add the URL of the image you'd like to use to display it below. If you have a local image you'd like to use, you can specify that in the following steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_X5dS1Rye2Q"
      },
      "outputs": [],
      "source": [
        "!wget https://storage.googleapis.com/cloud-samples-data/generative-ai/image/flowers.png"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGtJsg-MPSXi"
      },
      "source": [
        "If you'd like to use a different local image, modify the file name in `starting_image`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cl_lTdvEIt8k"
      },
      "outputs": [],
      "source": [
        "starting_image = \"flowers.png\"  # @param {type: 'string'}\n",
        "\n",
        "# Display the image\n",
        "fig, axis = plt.subplots(1, 1, figsize=(12, 6))\n",
        "axis.imshow(img.imread(starting_image))\n",
        "axis.set_title(\"Starting Frame\")\n",
        "axis.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsHmRk-_NYya"
      },
      "source": [
        "#### [Optional] Optimize your prompt: Image-to-video\n",
        "\n",
        "By considering the following options in your prompt, you can use Veo to create higher quality videos that more closely resemble your desired outcome. Learn more about advanced [prompting techniques for Veo 3](https://medium.com/google-cloud/veo-3-a-detailed-prompting-guide-867985b46018). To get started specify the following, or leave them as `None` if they don't align with your specific goals.\n",
        "- **Camera motion:** The camera movement that occurs while the rest of the scene remains static\n",
        "- **Subject animation:** Main character or object movement  \n",
        "- **Environmental animation:** Background or atmosphere movement\n",
        "- **Audio:** Various sound effects or dialogue that guides the visuals through sound"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6UE7AfftqSM5"
      },
      "outputs": [],
      "source": [
        "camera_motion = \"Extreme Close-Up\"  # @param [\"None\", \"Eye-Level Shot\", \"Low-Angle Shot\", \"High-Angle Shot\", \"Bird's-Eye View\", \"Top-Down Shot\", \"Worm's-Eye View\", \"Dutch Angle\", \"Canted Angle\", \"Close-Up\", \"Extreme Close-Up\", \"Medium Shot\", \"Full Shot\", \"Long Shot\", \"Wide Shot\", \"Establishing Shot\", \"Over-the-Shoulder Shot\", \"Point-of-View (POV) Shot\", \"Static Shot (or fixed)\", \"Pan (left)\", \"Pan (right)\", \"Tilt (up)\", \"Tilt (down)\", \"Dolly (In)\", \"Dolly (Out)\", \"Zoom (In)\", \"Zoom (Out)\", \"Truck (Left)\", \"Truck (Right)\", \"Pedestal (Up)\", \"Pedestal (Down)\", \"Crane Shot\", \"Aerial Shot\", \"Drone Shot\", \"Handheld\", \"Shaky Cam\", \"Whip Pan\", \"Arc Shot\", \"Wide-Angle Lens (e.g., 24mm)\", \"Telephoto Lens (e.g., 85mm)\", \"Shallow Depth of Field\", \"Bokeh\", \"Deep Depth of Field\", \"Lens Flare\", \"Rack Focus\", \"Fisheye Lens Effect\", \"Vertigo Effect (Dolly Zoom)\"]\n",
        "\n",
        "subject_animation = \"None\"  # @param [\"None\", \"The subject's head turns slowly\", \"The subject blinks slowly\", \"The subject's hair and clothes flutter gently in the wind\", \"A subtle smile appears on the subject's face\"]\n",
        "environmental_animation = \"Light changes subtly\"  # @param [\"None\", \"Fog rolls in slowly\", \"Rain starts to fall gently\", \"Leaves rustle in the wind\", \"Light changes subtly\", \"Reflections move on water\"]\n",
        "\n",
        "sound_effects = \"None\"  # @param [\"None\", \"Sound of a phone ringing\", \"Water splashing\", \"Soft house sounds\", \"Ticking clock\", \"City traffic and sirens\", \"Waves crashing\", \"Quiet office hum\"]\n",
        "dialogue = \"\"  # @param {type: 'string'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nG4tknxOR70c"
      },
      "source": [
        "Now, you'll use Gemini to take all of these keywords and combine them into a detailed Veo prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1NvfHEHrx2m"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\n",
        "\n",
        "keywords = []\n",
        "optional_keywords = [\n",
        "    camera_motion,\n",
        "    subject_animation,\n",
        "    environmental_animation,\n",
        "    sound_effects,\n",
        "]\n",
        "for keyword in optional_keywords:\n",
        "    if keyword != \"None\":\n",
        "        keywords.append(keyword)\n",
        "if dialogue != \"\":\n",
        "    keywords.append(dialogue)\n",
        "\n",
        "gemini_prompt = f\"\"\"\n",
        "You are an expert prompt engineer for Google's Veo model. Analyze the provided image and combine its content with the following motion and audio keywords to generate a single, cohesive, and cinematic prompt. Integrate the image's subject and scene with the requested motion and audio effects. The final output must be ONLY the prompt itself, with no preamble. Mandatory Keywords: {\",\".join(keywords)}\n",
        "\"\"\"\n",
        "with open(starting_image, \"rb\") as f:\n",
        "    image = f.read()\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=gemini_model,\n",
        "    contents=[gemini_prompt, types.Part.from_bytes(data=image, mime_type=\"image/png\")],\n",
        ")\n",
        "\n",
        "# Set Gemini's response in a prompt variable\n",
        "prompt = response.text\n",
        "display(Markdown(response.text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCc77j59KjGN"
      },
      "source": [
        "#### Send the video request\n",
        "\n",
        "If you're generating a video from an image you don't need to provide a prompt. The model will simply add motion to your image; however, a detailed prompt will help with video quality.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thPYb0eNtwxN"
      },
      "outputs": [],
      "source": [
        "if prompt == \"\":\n",
        "    prompt = (\n",
        "        \"zoom out of the flower field, play whimsical music\"  # @param {type: 'string'}\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFgQYeXHnidx"
      },
      "outputs": [],
      "source": [
        "print(prompt)\n",
        "enhance_prompt = True  # @param {type: 'boolean'}\n",
        "generate_audio = True  # @param {type: 'boolean'}\n",
        "\n",
        "operation = client.models.generate_videos(\n",
        "    model=video_model,\n",
        "    prompt=prompt,\n",
        "    image=types.Image.from_file(location=starting_image),\n",
        "    config=types.GenerateVideosConfig(\n",
        "        aspect_ratio=\"16:9\",\n",
        "        number_of_videos=1,\n",
        "        duration_seconds=8,\n",
        "        person_generation=\"allow_adult\",\n",
        "        enhance_prompt=enhance_prompt,\n",
        "        generate_audio=generate_audio,\n",
        "    ),\n",
        ")\n",
        "\n",
        "while not operation.done:\n",
        "    time.sleep(15)\n",
        "    operation = client.operations.get(operation)\n",
        "    print(operation)\n",
        "\n",
        "if operation.response:\n",
        "    show_video(operation.result.generated_videos[0].video.video_bytes)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "veo3_video_generation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}